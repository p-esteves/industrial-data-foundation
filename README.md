# Industrial Data Foundation

Projeto de refer√™ncia para implementa√ß√£o de pipelines de dados locais utilizando **Apache Airflow**. Este reposit√≥rio demonstra um fluxo ETL completo (Extra√ß√£o, Transforma√ß√£o e Carga) simulando o processamento de dados do CAGED (Cadastro Geral de Empregados e Desempregados), com foco em arquitetura audit√°vel, c√≥digo limpo e boas pr√°ticas de Engenharia de Dados.

## üéØ Contexto e Objetivo

O objetivo deste projeto √© fornecer um artefato t√©cnico que demonstre o dom√≠nio da orquestra√ß√£o de dados em ambiente local (Linux/WSL). O pipeline resolve o problema cl√°ssico de ingest√£o de dados brutos e sua disponibiliza√ß√£o otimizada para an√°lise.

**Destaques T√©cnicos:**
* Orquestra√ß√£o robusta com Airflow.
* Implementa√ß√£o da arquitetura **Medallion** (Camadas Bronze e Silver).
* Armazenamento otimizado em **Parquet** com particionamento Hive.
* Valida√ß√£o de qualidade de dados via script de auditoria.

## üèóÔ∏è Arquitetura do Pipeline

O fluxo de dados √© linear e determin√≠stico, projetado para garantir idempot√™ncia e rastreabilidade:

1.  **Ingest√£o (Extract):** Gera√ß√£o controlada de dados sint√©ticos simulando a fonte oficial do CAGED. Os dados brutos s√£o persistidos na camada **Bronze** em formato `.csv` (Raw).
2.  **Processamento (Transform):** Leitura da camada Bronze, aplica√ß√£o de tipagem forte (Casting), limpeza de dados (remo√ß√£o de outliers e registros inconsistentes) e transforma√ß√£o para formato colunar.
3.  **Armazenamento (Load):** Escrita na camada **Silver** em formato **Parquet**, utilizando compress√£o Snappy e particionamento f√≠sico por Estado (UF). Isso habilita o *partition pruning* em leituras futuras.
4.  **Auditoria (Quality Check):** Task final que valida a exist√™ncia f√≠sica dos arquivos, integridade das parti√ß√µes e consist√™ncia do schema gerado.

## üõ†Ô∏è Stack Tecnol√≥gica

As escolhas tecnol√≥gicas priorizam a execu√ß√£o "baterias inclusas" (baixo overhead) com ferramentas padr√£o de mercado:

*   **Apache Airflow (2.10.x):** Padr√£o da ind√∫stria para orquestra√ß√£o baseada em c√≥digo (Python).
*   **Python 3.12:** Linguagem core da Engenharia de Dados.
*   **Pandas & PyArrow:** Para manipula√ß√£o em mem√≥ria e escrita eficiente de formatos colunares.
*   **Linux (WSL2):** Ambiente nativo de execu√ß√£o do Airflow.

> **Justificativa:** A utiliza√ß√£o do Airflow em modo Standalone elimina a necessidade de containers Docker pesados para valida√ß√£o funcional, mantendo a complexidade focada na l√≥gica do pipeline e n√£o na infraestrutura.

## üöÄ Instru√ß√µes de Execu√ß√£o (Local)

Pr√©-requisitos: Ambiente Linux (Ubuntu/WSL2) e Python 3 instalados.

### 1. Configura√ß√£o do Ambiente
```bash
# 1. Clone o reposit√≥rio
git clone https://github.com/p-esteves/industrial-data-foundation.git
cd industrial-data-foundation

# 2. Crie e ative um ambiente virtual
python3 -m venv venv
source venv/bin/activate

# 3. Instale as depend√™ncias
pip install -r requirements.txt
```

### 2. Inicializa√ß√£o do Airflow
Configure o diret√≥rio home e inicialize o banco de dados local (SQLite):

```bash
export AIRFLOW_HOME=~/airflow

# Instala√ß√£o/Inicializa√ß√£o modo Standalone (recomendado para dev)
airflow standalone
```
*O comando acima inicializar√° o banco, criar√° um usu√°rio admin e subir√° os servi√ßos (Webserver e Scheduler). Anote a senha gerada no terminal.*

### 3. Deploy da DAG
Em um novo terminal (com o venv ativo e AIRFLOW_HOME definido):

```bash
# Crie a pasta de DAGs se n√£o existir
mkdir -p ~/airflow/dags

# Copie a DAG do projeto para o diret√≥rio do Airflow
cp dags/caged_etl.py ~/airflow/dags/
```

### 4. Execu√ß√£o do Pipeline
1.  Acesse a interface web em `http://localhost:8080`.
2.  Fa√ßa login (usu√°rio `admin` e senha gerada no passo 2).
3.  Localize a DAG `industrial-data-foundation`.
4.  Ative a DAG (toggle switch ON) e clique no bot√£o ‚ñ∂Ô∏è (Trigger DAG).

## üîé Valida√ß√£o e Resultados

Ap√≥s a conclus√£o da DAG (todas as tasks verdes), execute o script de valida√ß√£o local para auditar o Data Lake gerado:

```bash
python ler_lake.py
```

**Resultado esperado:**
*   Relat√≥rio listando as parti√ß√µes criadas (ex: `uf=SP`, `uf=CE`).
*   Contagem de arquivos Parquet e tamanho em disco.
*   Exibi√ß√£o do schema detectado e amostra dos dados.

## ‚ö†Ô∏è Limita√ß√µes e Assun√ß√µes

*   **Escopo de Demonstra√ß√£o:** O projeto foca na orquestra√ß√£o e estrutura√ß√£o de dados. A camada *Gold* (agrega√ß√µes de neg√≥cio) n√£o foi inclu√≠da intencionalmente para manter o escopo focado na funda√ß√£o dos dados.
*   **Schema Fixo:** Assume-se que a fonte de dados mant√©m contrato est√°vel. Em produ√ß√£o, seria necess√°rio um *Schema Registry* ou valida√ß√£o de contrato mais robusta.
*   **Armazenamento Local:** O Data Lake reside no filesystem local (`~/airflow/datalake`). Em produ√ß√£o, isso seria substitu√≠do por S3, GCS ou Azure Blob Storage alterando apenas a vari√°vel `BASE_DIR`.
