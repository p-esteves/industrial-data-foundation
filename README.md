# Industrial Data Foundation

Projeto de refer√™ncia para implementa√ß√£o de pipelines de dados locais utilizando **Apache Airflow**. Este reposit√≥rio demonstra um fluxo ETL completo (Extra√ß√£o, Transforma√ß√£o e Carga) simulando o processamento de dados do CAGED (Cadastro Geral de Empregados e Desempregados), com foco em arquitetura audit√°vel, c√≥digo limpo e boas pr√°ticas de Engenharia de Dados.

## üéØ Contexto e Objetivo

O objetivo deste projeto √© fornecer um artefato t√©cnico que demonstre o dom√≠nio da orquestra√ß√£o de dados em ambiente local (Linux/WSL). O pipeline resolve o problema cl√°ssico de ingest√£o de dados brutos e sua disponibiliza√ß√£o otimizada para an√°lise.

**Destaques T√©cnicos:**
* Orquestra√ß√£o robusta com Airflow.
* Implementa√ß√£o da arquitetura **Medallion** (Camadas Bronze e Silver).
* Armazenamento otimizado em **Parquet** com particionamento Hive.
* Valida√ß√£o de qualidade de dados via script de auditoria.
* **Dockerizado:** Ambiente reprodut√≠vel com `docker-compose`.

## üèóÔ∏è Arquitetura do Pipeline

O fluxo de dados √© linear e determin√≠stico, projetado para garantir idempot√™ncia e rastreabilidade:

1.  **Ingest√£o (Extract):** Gera√ß√£o controlada de dados sint√©ticos simulando a fonte oficial do CAGED. Os dados brutos s√£o persistidos na camada **Bronze** em formato `.csv` (Raw).
2.  **Processamento (Transform):** Leitura da camada Bronze, aplica√ß√£o de tipagem forte (Casting), limpeza de dados (remo√ß√£o de outliers e registros inconsistentes) e transforma√ß√£o para formato colunar.
3.  **Armazenamento (Load):** Escrita na camada **Silver** em formato **Parquet**, utilizando compress√£o Snappy e particionamento f√≠sico por Estado (UF). Isso habilita o *partition pruning* em leituras futuras.
4.  **Auditoria (Quality Check):** Task final que valida a exist√™ncia f√≠sica dos arquivos, integridade das parti√ß√µes e consist√™ncia do schema gerado.

## üõ†Ô∏è Stack Tecnol√≥gica

As escolhas tecnol√≥gicas priorizam a execu√ß√£o "baterias inclusas" (baixo overhead) com ferramentas padr√£o de mercado:

*   **Apache Airflow (2.10.x):** Padr√£o da ind√∫stria para orquestra√ß√£o baseada em c√≥digo (Python).
*   **Docker & Docker Compose:** Para isolamento e reprodutibilidade do ambiente.
*   **Python 3.12:** Linguagem core da Engenharia de Dados.
*   **Pandas & PyArrow:** Para manipula√ß√£o em mem√≥ria e escrita eficiente de formatos colunares.
*   **PostgreSQL:** Banco de metadados do Airflow (no ambiente Docker).

> **Justificativa:** A utiliza√ß√£o do Airflow em modo Standalone elimina a necessidade de containers Docker pesados para valida√ß√£o funcional, mantendo a complexidade focada na l√≥gica do pipeline e n√£o na infraestrutura.

## üöÄ Como Executar

### Op√ß√£o 1: Via Docker (Recomendado)
Ideal para avalia√ß√£o r√°pida e limpa, sem instalar depend√™ncias no seu sistema.

1.  **Inicie o ambiente:**
    ```bash
    docker-compose up -d
    ```
    *Aguarde alguns instantes at√© que os servi√ßos (Webserver, Scheduler, Postgres) estejam saud√°veis.*

2.  **Acesse a interface:**
    *   URL: `http://localhost:8080`
    *   Login: `admin` / `admin`

3.  **Execute o Pipeline:**
    *   Ative a DAG `industrial-data-foundation` (Toggle ON).
    *   Clique em "Trigger DAG" (‚ñ∂Ô∏è).

### Op√ß√£o 2: Local (Python Nativo)
Recomendado para desenvolvimento se voc√™ j√° possui ambiente Linux/WSL configurado.

1.  **Setup:**
    ```bash
    python3 -m venv venv
    source venv/bin/activate
    pip install -r requirements.txt
    
    export AIRFLOW_HOME=~/airflow
    airflow standalone
    ```

2.  **Deploy:**
    ```bash
    mkdir -p ~/airflow/dags
    cp dags/caged_etl.py ~/airflow/dags/
    ```

## üîé Valida√ß√£o e Resultados

Ap√≥s a conclus√£o da DAG (todas as tasks verdes), execute o script de valida√ß√£o local para auditar o Data Lake gerado:


**Se rodou via Docker:**
```bash
# Executa o script python usando o ambiente do container
docker-compose run --rm airflow-webserver python ler_lake.py
```

**Se rodou Localmente:**
```bash
python ler_lake.py
```

**Resultado esperado:**
*   Relat√≥rio listando as parti√ß√µes criadas (ex: `uf=SP`, `uf=CE`).
*   Contagem de arquivos Parquet e tamanho em disco.
*   Exibi√ß√£o do schema detectado e amostra dos dados.

## ‚ö†Ô∏è Limita√ß√µes e Assun√ß√µes

*   **Escopo de Demonstra√ß√£o:** O projeto foca na orquestra√ß√£o e estrutura√ß√£o de dados. A camada *Gold* (agrega√ß√µes de neg√≥cio) n√£o foi inclu√≠da intencionalmente para manter o escopo focado na funda√ß√£o dos dados.
*   **Schema Fixo:** Assume-se que a fonte de dados mant√©m contrato est√°vel. Em produ√ß√£o, seria necess√°rio um *Schema Registry* ou valida√ß√£o de contrato mais robusta.
*   **Armazenamento Local:** O Data Lake reside no filesystem local (`~/airflow/datalake`). Em produ√ß√£o, isso seria substitu√≠do por S3, GCS ou Azure Blob Storage alterando apenas a vari√°vel `BASE_DIR`.

## üß™ Testes Automatizados

O projeto inclui uma su√≠te de testes de simula√ß√£o que valida a l√≥gica ETL (Extra√ß√£o, Transforma√ß√£o e Carga) isoladamente, sem necessidade de subir toda a infraestrutura do Airflow. √ötil para CI/CD ou ambientes de desenvolvimento restritos.

**Para rodar a simula√ß√£o:**

Linux/Mac:
```bash
python tests/simulate_pipeline.py
```

Windows (Script Autom√°tico):
```cmd
tests\run_test.bat
```
