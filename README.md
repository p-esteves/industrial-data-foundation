# ğŸ­ Industrial Data Foundation (ETL Pipeline)

Pipeline de Engenharia de Dados construÃ­do com **Apache Airflow** para processamento de dados trabalhistas (CAGED). O projeto simula um ambiente de Big Data completo, desde a ingestÃ£o atÃ© a disponibilizaÃ§Ã£o em Data Lake particionado.

## ğŸ› ï¸ Tecnologias Utilizadas
* **OrquestraÃ§Ã£o:** Apache Airflow 2.10 (Standalone)
* **Linguagem:** Python 3.12
* **Processamento:** Pandas & Numpy
* **Storage (Data Lake):**
    * ğŸ¥‰ **Bronze:** Dados Brutos em CSV
    * ğŸ¥ˆ **Silver:** Dados Processados em Parquet (Particionamento Hive por UF)
* **Infraestrutura:** WSL2 (Ubuntu Linux)

## ğŸš€ Arquitetura do Pipeline

O fluxo de dados segue a arquitetura Medallion (Bronze/Silver):

1.  **Extract (IngestÃ£o):** GeraÃ§Ã£o e extraÃ§Ã£o de dados brutos simulando a base do CAGED.
2.  **Transform (Processamento):**
    * Limpeza de dados (remoÃ§Ã£o de ruÃ­dos/outliers).
    * ConversÃ£o de tipagem (Casting).
    * GravaÃ§Ã£o em formato colunar **Parquet** com compressÃ£o Snappy.
3.  **Load (Armazenamento):** Particionamento fÃ­sico dos arquivos por Estado (`uf=SP`, `uf=CE`, etc) para otimizaÃ§Ã£o de consultas.
4.  **Quality Check:** ValidaÃ§Ã£o automÃ¡tica da existÃªncia e integridade dos arquivos processados.

## ğŸ“‚ Estrutura do Projeto

```[text]
industrial-data-foundation/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ caged_etl.py      # CÃ³digo principal da DAG do Airflow
â”œâ”€â”€ ler_lake.py           # Script de validaÃ§Ã£o e leitura do Data Lake (Pandas)
â”œâ”€â”€ requirements.txt      # DependÃªncias do projeto
â””â”€â”€ README.md             # DocumentaÃ§Ã£o
```

ğŸ‘£ Como Executar
Clone o repositÃ³rio:
```[bash]
git clone https://github.com/p-esteves/industrial-data-foundation.git
```

Instale as dependÃªncias:
```[bash]
pip install -r requirements.txt
```

Configure o Airflow e mova a DAG para a pasta de dags local.

Execute o script de validaÃ§Ã£o para ler o Data Lake:bash
```[bash]
python ler_lake.py
```
